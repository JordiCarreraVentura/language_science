{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "found-investigator",
   "metadata": {},
   "source": [
    "# ReFinED use case\n",
    "\n",
    "This notebook documents multiple executions of the code in `refined_test.py` and summarizes the main findings.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "1. Great tool. WSD and entity linking from scratch. NER presupposed.\n",
    "2. Results:\n",
    "   1. **Apparently** good results with the Venice example (invariant entities, conventional entities, general-domain NER and WSD).\n",
    "   2. Massive drop on the titles dataset.\n",
    "      - Possibly due to non-natural text lacking a canonical linguistic input structure for the part of speech tagging and NER processes. As a result, few entities are detected and disambiguated.\n",
    "3. NER is not optimal, tokenizer seems an issue.\n",
    "4. WSD is okay but not impressive.\n",
    "5. Performance (throughout) is not great given the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-college",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "optimum-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "precise-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-hayes",
   "metadata": {},
   "source": [
    "The `WikipediaAnnotator` class implements an entity extractor that uses fuzzy string matching to retrieve candidate entities from Wikipedia. We expect this to provide a relatively strong baseline but it is uncompetitive as of now, given that it leverages Wikipedia's public search API, which would not be possible in a production environment, as well as cached look-up, which would be acceptable in any production setting but is actually indispensable to run the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "failing-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia_annotator import WikipediaAnnotator\n",
    "BASELINE = WikipediaAnnotator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-mainland",
   "metadata": {},
   "source": [
    "Target system: ReFinED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "provincial-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refined_test import ReFinED\n",
    "RFED = ReFinED()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-liberal",
   "metadata": {},
   "source": [
    "The test examples are defined in a separate module both for tracking and convenience and are imported in the cell below. These texts have been manually hand-picked and they meet no specific set of selection criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "precious-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from texts import TEXT__PAPER_TITLES, TEXT_VENICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-tunisia",
   "metadata": {},
   "source": [
    "We also need evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "proof-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import character_coverage\n",
    "from evaluation import diff_annotations\n",
    "from evaluation import f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-square",
   "metadata": {},
   "source": [
    "##Â _Venice_ text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-bicycle",
   "metadata": {},
   "source": [
    "Let us check the results for the text about the city of Venice first. We start with the baseline annotation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "greek-thomson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 9th to the 12th centuries, Venice developed into a powerful maritime empire (an Italian thalassocracy known also as repubblica marinara). In addition to Venice there were seven others: the most important ones were Genoa, Pisa, and Amalfi; and the lesser known were Ragusa, Ancona, Gaeta and Noli. Its own strategic position at the head of the Adriatic made Venetian naval and commercial power almost invulnerable. With the elimination of pirates along the Dalmatian coast, the city became a flourishing trade centre between Western Europe and the rest of the world, especially with the Byzantine Empire and Asia, where its navy protected sea routes against piracy. The Republic of Venice seized a number of places on the eastern shores of the Adriatic before 1200, mostly for commercial reasons, because pirates based there were a menace to trade. The doge already possessed the titles of Duke of Dalmatia and Duke of Istria. Later mainland possessions, which extended across Lake Garda as far west as the Adda River, were known as the Terraferma; they were acquired partly as a buffer against belligerent neighbours, partly to guarantee Alpine trade routes, and partly to ensure the supply of mainland wheat (on which the city depended). In building its maritime commercial empire, Venice dominated the trade in salt, acquired control of most of the islands in the Aegean, including Crete, and Cyprus in the Mediterranean, and became a major power-broker in the Near East. By the standards of the time, Venice's stewardship of its mainland territories was relatively enlightened and the citizens of such towns as Bergamo, Brescia, and Verona rallied to the defence of Venetian sovereignty when it was threatened by invaders.\n"
     ]
    }
   ],
   "source": [
    "print(TEXT_VENICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "becoming-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordi/Laboratorio/Python/language_science/ReFinED/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning:\n",
      "\n",
      "User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_entities_venice = BASELINE.extract_terms(TEXT_VENICE)\n",
    "\n",
    "rfed_entities_venice = RFED.extract_terms(TEXT_VENICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "banned-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline coverage   0.29\n",
      "test coverage       0.18\n"
     ]
    }
   ],
   "source": [
    "baseline_coverage = character_coverage(TEXT_VENICE, baseline_entities_venice)\n",
    "test_coverage = character_coverage(TEXT_VENICE, rfed_entities_venice)\n",
    "print(f'baseline coverage{baseline_coverage:7.2f}\\ntest coverage {test_coverage:10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "gothic-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_diff_venice, rfed_diff_venice = diff_annotations(baseline_entities_venice, rfed_entities_venice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "golden-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 84, 'empire', 'Empire')\n",
      "(150, 158, 'addition', 'Addition')\n",
      "(340, 344, 'head', 'Head')\n"
     ]
    }
   ],
   "source": [
    "for x in baseline_diff_venice[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "subject-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 144, 'repubblica marinara', 'Maritime republics')\n",
      "(274, 280, 'Ragusa', 'Ragusa, Sicily')\n",
      "(352, 360, 'Adriatic', 'Adriatic Sea')\n"
     ]
    }
   ],
   "source": [
    "for x in rfed_diff_venice[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-exhibition",
   "metadata": {},
   "source": [
    "The baseline's Recall is 11% higher than the ReFinED pipeline (61% higher in relative terms), which is substantial. However, out of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "modern-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline_diff_venice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-renaissance",
   "metadata": {},
   "source": [
    "total entities, 29 were found to be correct, and the remaining 14 were false positives. These could be further categorized into two main error types:\n",
    "1. Verbs\n",
    "2. Generic nouns\n",
    "\n",
    "This results in a Precision of 67%, which stands in contrast with ReFinED's Precision, which was determined to be a substantial 91% after manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "other-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "venice_baseline_recall = 0.29\n",
    "venice_test_recall = 0.18\n",
    "\n",
    "venice_baseline_precision = 0.67\n",
    "venice_test_precision = 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-chassis",
   "metadata": {},
   "source": [
    "Considering these estimates Precision and Recall, their respective F-$\\beta$ scores with $\\beta$ = `0.5, 1.0, 2.0` are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-concord",
   "metadata": {},
   "source": [
    "**F-$\\beta$ score with $\\beta$** = _0.5_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "refined-metro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline F_beta =   0.53\n",
      "test F_beta =       0.50\n"
     ]
    }
   ],
   "source": [
    "f = 0.5\n",
    "baseline_f = f_score(venice_baseline_precision, venice_baseline_recall, f=f)\n",
    "test_f = f_score(venice_test_precision, venice_test_recall, f=f)\n",
    "print(f'baseline F_beta = {baseline_f:6.2f}\\ntest F_beta = {test_f:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-exhibit",
   "metadata": {},
   "source": [
    "**F-$\\beta$ score with $\\beta$** = _1.0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "necessary-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline F_beta =   0.40\n",
      "test F_beta =       0.30\n"
     ]
    }
   ],
   "source": [
    "f = 1.0\n",
    "baseline_f = f_score(venice_baseline_precision, venice_baseline_recall, f=f)\n",
    "test_f = f_score(venice_test_precision, venice_test_recall, f=f)\n",
    "print(f'baseline F_beta = {baseline_f:6.2f}\\ntest F_beta = {test_f:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-target",
   "metadata": {},
   "source": [
    "**F-$\\beta$ score with $\\beta$** = _2.0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "urban-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline F_beta =   0.33\n",
      "test F_beta =       0.21\n"
     ]
    }
   ],
   "source": [
    "f = 2.0\n",
    "baseline_f = f_score(venice_baseline_precision, venice_baseline_recall, f=f)\n",
    "test_f = f_score(venice_test_precision, venice_test_recall, f=f)\n",
    "print(f'baseline F_beta = {baseline_f:6.2f}\\ntest F_beta = {test_f:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-tiger",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "\n",
    "If the two error categories detected during the error analysis were handled, the baseline system may decrease the number of false positives. Each category comprised 7 cases. If both categories are resolved, the error rate would lowered by twice as much.\n",
    "\n",
    "Out of the two error categories, **verbs** might be easier to handle using part-of-speech tagging and restricting Wikipedia look-up to noun phrases.\n",
    "\n",
    "The second error type, **generic nouns**, could be addressed either by extending the list of stopwords manually (easy, but it may require maintenance), implementing word sense disambiguation, or using some type of context-based filtering. Generally speaking, single terms above a certain frequency should either be avoided entirely, or should undergo further validation before being extracted.\n",
    "\n",
    "More specifically, any single words that, after being looked up on Wikipedia, result in a list with multiple candidate senses whose Wikipedia articles all have a similar title after removing any parentheticals (likely after triggering a `DisambiguationError`), should be assigned the sense with the highest semantic similarity to the term's current context of occurrence. The semantic similarity measurement could be done between the context, as the target, and any of the following sources of information for the candidate (non-exhaustive list):\n",
    "- the term's Wikipedia summary\n",
    "- each of the term's Wikipedia categories\n",
    "- entities from pages linked to the term's\n",
    "- the term's full Wikipedia page\n",
    "- and so on.\n",
    "\n",
    "If both **verb** and **generic noun** errors were successfully handled and all false positives were removed, the top maximum performance expected for the baseline setting would increase by 14% up to 67% for $\\beta = 0.5$ (26% relative improvement) and by 5% up to 45% for $\\beta = 1.0$ (12.5% relative improvement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "stupid-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline F_beta =   0.67\n",
      "test F_beta =       0.50\n",
      "\n",
      "baseline F_beta =   0.45\n",
      "test F_beta =       0.30\n",
      "\n",
      "baseline F_beta =   0.34\n",
      "test F_beta =       0.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "venice_baseline_precision_revised = 1.0\n",
    "\n",
    "for f in [0.5, 1.0, 2.0]:\n",
    "    baseline_f = f_score(venice_baseline_precision_revised, venice_baseline_recall, f=f)\n",
    "    test_f = f_score(venice_test_precision, venice_test_recall, f=f)\n",
    "    print(f'baseline F_beta = {baseline_f:6.2f}\\ntest F_beta = {test_f:10.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-nitrogen",
   "metadata": {},
   "source": [
    "## AI/NLP titles texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "intelligent-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models. Rumour Evaluation with Very Large Language Models. A Legal Framework for Natural Language Processing Model Training in Portugal. Investigating Automatic Scoring and Feedback using Large Language Models. When Quantization Affects Confidence of Large Language Models. Better & Faster Large Language Models via Multi-token Prediction. Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models. Computational Job Market Analysis with Natural Language Processing. A Survey of Generative Search and Recommendation in the Era of Large Language Models. Octopus v4: Graph of language models. Utilizing Large Language Models to Identify Reddit Users Considering Vaping Cessation for Digital Interventions. BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers. ChatGPT Is Here to Help, Not to Replace Anybody -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses. Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement. MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition. In-Context Learning with Long-Context Models: An In-Depth Exploration. Structure learning of Hamiltonians from real-time evolution. Koopman-based Deep Learning for Nonlinear System Estimation. A Survey on Deep Active Learning: Recent Advances and New Frontiers. Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge. New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese Multimodal Aspect-Category Sentiment Analysis. CLARE: Cognitive Load Assessment in REaltime with Multimodal Data. Context-Aware Machine Translation with Source Coreference Explanation. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. Retrieval-Oriented Knowledge for Click-Through Rate Prediction. Gradient-based Automatic Per-Weight Mixed Precision Quantization for Neural Networks On-Chip. Automatic Creative Selection with Cross-Modal Matching. Misaka: Interactive Swarm Testbed for Smart Grid Distributed Algorithm Test and Evaluation. Text Sentiment Analysis and Classification Based on Bidirectional Gated Recurrent Units (GRUs) Model. Derivative-based regularization for regression. NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and Encoder-based Scoring for Semantic Textual Relatedness. Joint Optimization of Piecewise Linear Ensembles. A Scoping Review on Simulation-based Design Optimization in Marine Engineering: Trends, Best Practices, and Gaps. KVP10k : A Comprehensive Dataset for Key-Value Pair Extraction in Business Documents. Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment. M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework. The Future of Scientific Publishing: Automated Article Generation. Time Machine GPT. Interest Clock: Time Perception in Real-Time Streaming Recommendation System. Humans prefer interacting with slow, less realistic butterfly simulations. Modeling Orthographic Variation in Occitan's Dialects. InspectorRAGet: An Introspection Platform for RAG Evaluation. Edge Importance in Complex Networks. MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot. A Comprehensive Survey of Dynamic Graph Neural Networks: Models, Frameworks, Benchmarks, Experiments and Challenges. GenAI Distortion: The Effect of GenAI Fluency and Positive Affect. Winning the Social Media Influence Battle: Uncertainty-Aware Opinions to Understand and Spread True Information via Competitive Influence Maximization. A Formal Specification of a Data Model for Malaria Surveillance in the Developing World. \"Actually I Can Count My Blessings\": User-Centered Design of an Application to Promote Gratitude Among Young Adults. Exploring Weighted Property Approaches for RDF Graph Similarity Measure. Dynamic Human Trust Modeling of Autonomous Agents With Varying Capability and Strategy. Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking. Capabilities\n"
     ]
    }
   ],
   "source": [
    "print(TEXT__PAPER_TITLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "sensitive-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_entities_paper = BASELINE.extract_terms(TEXT__PAPER_TITLES)\n",
    "\n",
    "rfed_entities_paper = RFED.extract_terms(TEXT__PAPER_TITLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "assured-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline coverage   0.45\n",
      "test coverage       0.05\n"
     ]
    }
   ],
   "source": [
    "baseline_coverage = character_coverage(TEXT__PAPER_TITLES, baseline_entities_paper)\n",
    "test_coverage = character_coverage(TEXT__PAPER_TITLES, rfed_entities_paper)\n",
    "print(f'baseline coverage{baseline_coverage:7.2f}\\ntest coverage {test_coverage:10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-teddy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refined",
   "language": "python",
   "name": "refined"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a28a45-1759-44aa-966a-c77bec19b6de",
   "metadata": {},
   "source": [
    "# Ranking metrics\n",
    "\n",
    "1. [Online metrics](#online-metrics)\n",
    "   1. [Hit Ratio](#hit-ratio)\n",
    "2. [Offline metrics](#offline-metrics)\n",
    "   1. [Reciprocal Rank and Mean Reciprocal Rank](#reciprocal-rank-and-mean-reciprocal-rank)\n",
    "   2. [Mean Average Precision](#mean-average-precision-map)\n",
    "3. [Examples](#examples)\n",
    "4. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb2e35-e938-47e2-b7c2-a836211ef47f",
   "metadata": {},
   "source": [
    "## Online metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330f9d8-9903-478e-aa4a-9e7846613866",
   "metadata": {},
   "source": [
    "### Hit Ratio\n",
    "\n",
    "[Source 1](#reference_1)\n",
    "\n",
    "> the fraction of users for which the correct answer is included in the recommendation list of length $L$.\n",
    "\n",
    "$D = \\text{the superset containing every set of recommendations }d\\text{ served to a user, such that }|d| = L$\n",
    "\n",
    "$y = \\text{the correct answer}$\n",
    "\n",
    "$$HR_L = \\frac{|d: d \\in D \\land y \\in d|}{|D|}$$\n",
    "\n",
    "**NOTE**: $L$ is a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d35c4a-c719-46b6-a51e-1e508b8a1b8d",
   "metadata": {},
   "source": [
    "## Offline metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec4d19-5199-40ab-ad6f-81e1c7a3f509",
   "metadata": {},
   "source": [
    "### Reciprocal Rank and Mean Reciprocal Rank\n",
    "\n",
    "[Source 1](#reference_1), [Back to top](#ranking-metrics)\n",
    "\n",
    "$D = \\text{the superset containing every ranked set of recommendations }d\\text{ served to a user, such that }|d| = L$\n",
    "\n",
    "$$RR(d) = \\sum\\limits_{i: 1 ≤ i ≤ L} \\frac{relevance_i}{rank_i}$$\n",
    "\n",
    "$$MRR(D) = \\frac{ \\sum\\limits_{i = 1}^{|D|} RR(D_i) }{|D|} $$\n",
    "\n",
    "**NOTE**: _one could argue that hit ratio is actually a special case of MRR in which RR(d) is binary, as it becomes 1 if there is a relevant item in the list, 0 otherwise._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80531ea4-5dfb-431d-b8d8-499e8fc29fe2",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)\n",
    "\n",
    "[Source 1](#reference_1), [Back to top](#ranking-metrics)\n",
    "\n",
    "$K = \\text{the maximum number of top elements we want to consider}$\n",
    "\n",
    "$k = \\text{the number of top elements we want to consider to calculate metrics such that } 1 ≤ k ≤ K$\n",
    "\n",
    "$D = \\text{the superset containing every ranked list of recommendations }d\\text{ served to a user, such that }|d| = k$\n",
    "\n",
    "$$AP(D_i) = \\sum\\limits_{k = 1}^{K} \\text{Precision@}k(D_i) \\times RelevanceMask_i$$\n",
    "\n",
    "$$MAP(D) = \\frac{\\sum\\limits_{i = 1}^{|D|} AP(D_i)}{|D|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fb03f-bab8-4b4f-b04c-e887fbbe6d4a",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "[Back to top](#ranking-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09e53a1-37bd-4460-9bc4-1b7c1bb1b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 0, 5, 7, 4, 9, 1, 2, 3, 6]\n",
      "[8, 0, 5, 7, 4, 9, 1, 2, 3, 6]\n",
      "{8: 1, 0: 1, 5: 1, 7: 1, 4: 1, 9: 0, 1: 0, 2: 0, 3: 0, 6: 0}\n",
      "\n",
      "[5, 3, 0, 6, 9, 2, 4, 7, 8, 1]\n",
      "[5, 3, 0, 6, 9, 2, 4, 7, 8, 1]\n",
      "{5: 1, 3: 1, 0: 1, 6: 1, 9: 1, 2: 0, 4: 0, 7: 0, 8: 0, 1: 0}\n",
      "\n",
      "[9, 0, 3, 1, 4, 8, 7, 2, 6, 5]\n",
      "[9, 0, 3, 1, 4, 8, 7, 2, 6, 5]\n",
      "{9: 1, 0: 1, 3: 1, 1: 1, 4: 1, 8: 0, 7: 0, 2: 0, 6: 0, 5: 0}\n",
      "\n",
      "[1, 0, 9, 5, 8, 6, 2, 7, 3, 4]\n",
      "[1, 0, 9, 5, 8, 6, 2, 7, 3, 4]\n",
      "{1: 1, 0: 1, 9: 1, 5: 1, 8: 1, 6: 0, 2: 0, 7: 0, 3: 0, 4: 0}\n",
      "\n",
      "[0, 7, 9, 4, 3, 1, 6, 2, 8, 5]\n",
      "[0, 7, 9, 4, 3, 1, 6, 2, 8, 5]\n",
      "{0: 1, 7: 1, 9: 1, 4: 1, 3: 1, 1: 0, 6: 0, 2: 0, 8: 0, 5: 0}\n",
      "\n",
      "[9, 6, 7, 0, 1, 8, 2, 5, 3, 4]\n",
      "[9, 6, 7, 0, 1, 8, 2, 5, 3, 4]\n",
      "{9: 1, 6: 1, 7: 1, 0: 1, 1: 1, 8: 0, 2: 0, 5: 0, 3: 0, 4: 0}\n",
      "\n",
      "[0, 8, 2, 6, 1, 5, 4, 3, 9, 7]\n",
      "[0, 8, 2, 6, 1, 5, 4, 3, 9, 7]\n",
      "{0: 1, 8: 1, 2: 1, 6: 1, 1: 1, 5: 0, 4: 0, 3: 0, 9: 0, 7: 0}\n",
      "\n",
      "[8, 9, 0, 7, 3, 1, 2, 6, 5, 4]\n",
      "[8, 9, 0, 7, 3, 1, 2, 6, 5, 4]\n",
      "{8: 1, 9: 1, 0: 1, 7: 1, 3: 1, 1: 0, 2: 0, 6: 0, 5: 0, 4: 0}\n",
      "\n",
      "[1, 3, 8, 4, 5, 6, 9, 7, 2, 0]\n",
      "[1, 3, 8, 4, 5, 6, 9, 7, 2, 0]\n",
      "{1: 1, 3: 1, 8: 1, 4: 1, 5: 1, 6: 0, 9: 0, 7: 0, 2: 0, 0: 0}\n",
      "\n",
      "[2, 6, 0, 5, 1, 7, 4, 9, 3, 8]\n",
      "[2, 6, 0, 5, 1, 7, 4, 9, 3, 8]\n",
      "{2: 1, 6: 1, 0: 1, 5: 1, 1: 1, 7: 0, 4: 0, 9: 0, 3: 0, 8: 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "movies = list(range(10))\n",
    "n_users = 10\n",
    "n_relevant = 5\n",
    "\n",
    "get_preferences = lambda x: random.sample(movies, len(movies))\n",
    "\n",
    "movie_preferences = [\n",
    "    get_preferences(u)\n",
    "    for u in range(n_users)\n",
    "]\n",
    "\n",
    "relevance_masks = []\n",
    "for mvps in movie_preferences:\n",
    "    relevance_mask = dict([])\n",
    "    for idx, mvp in enumerate(mvps):\n",
    "        if idx < n_relevant:\n",
    "            relevance_mask[mvp] = 1\n",
    "        else:\n",
    "            relevance_mask[mvp] = 0\n",
    "    relevance_masks.append(relevance_mask)\n",
    "\n",
    "accuracy = 0.8\n",
    "movie_recommendations = [\n",
    "    preferences if random.random() < accuracy\n",
    "    else random.sample(preferences, len(preferences))\n",
    "    for preferences in movie_preferences\n",
    "]\n",
    "\n",
    "for rm, mv, mr in zip(relevance_masks, movie_preferences, movie_recommendations):\n",
    "    print(mv)\n",
    "    print(mr)\n",
    "    print(rm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da63339f-cfe1-4a7f-97e0-76be9dc705de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_at_k(denom, y_true, y_pred, k=2, relevance_masks=[], rounding=4):\n",
    "    tp = 0\n",
    "    p = 0\n",
    "    t = 0\n",
    "    if relevance_masks:\n",
    "        for preferences, recommendations, relevance_mask in zip(y_true, y_pred, relevance_masks):\n",
    "            expected = {mv for mv in preferences[:k] if relevance_mask[mv]}\n",
    "            predicted = {mv for mv in recommendations[:k] if relevance_mask[mv]}\n",
    "            true_positives = expected.intersection(predicted)\n",
    "            tp += len(true_positives)\n",
    "            p += k\n",
    "            t += sum(relevance_mask.values())\n",
    "    else:\n",
    "        for preferences, recommendations in zip(y_true, y_pred):\n",
    "            expected = {mv for mv in preferences[:k]}\n",
    "            predicted = {mv for mv in recommendations[:k]}\n",
    "            true_positives = expected.intersection(predicted)\n",
    "            tp += len(true_positives)\n",
    "            p += k\n",
    "            t += len(expected)\n",
    "    return round(tp / p if denom == 'precision' else tp / t, rounding)\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=2, relevance_masks=[], rounding=4):\n",
    "   return metric_at_k('precision', y_true, y_pred, k=k, relevance_masks=relevance_masks, rounding=rounding)\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k=2, relevance_masks=[], rounding=4):\n",
    "   return metric_at_k('recall', y_true, y_pred, k=k, relevance_masks=relevance_masks, rounding=rounding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d44e62-e31b-4df4-be29-e0a0c473ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(precision_at_k(movie_preferences, movie_recommendations, 4))\n",
    "print(recall_at_k(movie_preferences, movie_recommendations, 4))\n",
    "print(recall_at_k(movie_preferences, movie_recommendations, 4, relevance_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9362e-d31b-4e3a-a3e0-a495e723782a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Back to top](#ranking-metrics)\n",
    "\n",
    "1. <a id=\"reference_1\"></a> [Ranking Evaluation Metrics for Recommender Systems](https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54)\n",
    "2. <a id=\"reference_2\"></a>[Demystifying NDCG](https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

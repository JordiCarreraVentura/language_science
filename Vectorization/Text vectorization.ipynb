{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-fashion",
   "metadata": {},
   "source": [
    "# Vectorization of short texts\n",
    "\n",
    "Currently, many NLP tasks receive as input tweets or tweet-like messages, that is, short texts that usually correspond to a single sentence. Think of e-commerce product titles, questions in question-answering systems, requests in intent detection, or individual sentences for a common use case of SEQ2SEQ and Transformer-based sentence encoders.\n",
    "\n",
    "However, standard bag-of-word (BoW) representations, as originally developed in the area of Information Retrieval and later adopted for Natural Language Processing, generally make the assumption that a system's input are whole documents, not isolated sentences, and longer documents providing a far larger context for language processing than sentences only.\n",
    "\n",
    "For instance, Latent-Dirichlet-Allocation (LDA)-based topic modelling was initially designed to represent the meaning of each word based on the meaning of its neighboring words. As such, long documents provided a much richer context from which to derive this kind of inferences, whereas short texts are usually too fragmented to allow or effective modelling using the same technique, and LDA is well-known to struggle with short documents.\n",
    "\n",
    "So, what is the specific impact of short texts on vectorization? If BoW representations are intended for longer texts, what happens if we suddenly start using them for shorter texts? Does it matter? Or does everything remain the same?\n",
    "\n",
    "In this notebook we want to show that it generally does not matter, except in one possible situation.\n",
    "\n",
    "For our experiment, we will need to\n",
    "1. define the dataset we will be using,\n",
    "2. define which text vectorization methods we want to compare;\n",
    "3. and assess the differences between each method when run on the given dataset.\n",
    "\n",
    "Points 1 and 2 will be covered next. After that, we will use them as inputs for the analysis in point 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-discipline",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "For our example, we will consider a small toy dataset consisting of 3 classes\n",
    "\n",
    "`Y = {\"cell-phones\", \"books\", \"nutrition\"}`\n",
    "\n",
    "with a few documents each:\n",
    "1. 9 documents for `cell-phones`\n",
    "2. 6 documents for `nutrition`\n",
    "3. 6 documents or `books`\n",
    "\n",
    "The dataset is hard-coded as variable `ads` inside the `Dataset` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-designation",
   "metadata": {},
   "source": [
    "## 2. Vectorizers\n",
    "\n",
    "We will compare four types of vectorization strategies:\n",
    "1. Dictionary vectorization\n",
    "2. Frequency vectorization\n",
    "3. TFIDF-weighted vectorization\n",
    "4. TFICF-weighted vectorization\n",
    "\n",
    "All of these strategies are implemented as Python classes in the `Vectorizer` module under the following names, respectively:\n",
    "1. `DictionaryVectorizer`\n",
    "2. `CountVectorizer`\n",
    "3. `TfidfVectorizer` and 4 (`TfidfVectorizer` with keyword argument `group_by_class` set to `True`)\n",
    "\n",
    "Here is the one-paragraph description of each of these methods:\n",
    "1. The `DictionaryVectorizer` will take each document and return its binary encoding, that is, a vector with as many columns as words are in our vocabulary and, for each column, either a 1 or a 0, depending on whether the document contains that word. This is esentially an implementation of the identity function over a vocabulary and over all the words in an input text (the same that we would get if using `scikit-learn.feature_extraction.text.CountVectorizer` class with the `binary` parameter set to `True`, for those who are familiar with Python's `scikit-learn` library).\n",
    "2. The `CountVectorizer` also returns a vector with as many columns as words are in our vocabulary but, for each column, the value is **not** 1 or 0, but actually the **frequency** or the number of times appears in the input text (where, remember, each column corresponds to one of the words in our vocabulary, and only that word).\n",
    "3. The `TfidfVectorizer`\n",
    "\n",
    "### Working hypothesis\n",
    "Based on these descriptions, we can already venture some hypotheses:\n",
    "1. `CountVectorizer` represents essentially the same information as the `DictionaryVectorizer` but with raw frequency counts instead of a categorial binary labeling. If we are working with long documents containing many mentions of the same words, then the values in `CountVectorizer`s vector would be much larger than the values in `DictionaryVectorizer`s vector, since the latter are effectively capped to 1, no matter how many times each word appears in the text. However, if we working with short texts, this no longer applies, since short texts tend not to contain any duplicated words, except for prepositions, determiners, and similar function words with little lexical meaning. Therefore, our hypothesis is that, with most words having the same frequency in short texts, a short text's `DictionaryVectorizer`-encoded vector must look very similar, or identical, to a `CountVectorizer`-encoded vector (this is definitly **not** true for long documents).\n",
    "\n",
    "\n",
    "\n",
    "I'm not using scikit-learn's versions because I prefer to implement some\n",
    "#Â custom functionality directly from scratch (scikit-learn's vectorizers could\n",
    "also be extended accordingly, though), notice the `group_by_class` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-homeless",
   "metadata": {},
   "source": [
    "## 3. Analysis\n",
    "\n",
    "### Python pipeline\n",
    "\n",
    "Below is the code for the experiment we will be running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dramatic-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ericsson DF688 Vintage Flip Cell Phone NEW LISTING Ericsson DF688 Vintage Flip Cell Phone\n",
      "cell-phones\n",
      "\t CountVectorizer\n",
      "\t\t2.00\tvintage\n",
      "\t\t2.00\tcell\n",
      "\t\t2.00\tphone\n",
      "\t\t2.00\tflip\n",
      "\t\t1.00\tnew\n",
      "\t\t1.00\tlisting\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tnew\n",
      "\t\t1.00\tvintage\n",
      "\t\t1.00\tcell\n",
      "\t\t1.00\tphone\n",
      "\t\t1.00\tlisting\n",
      "\t\t1.00\tflip\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tflip\n",
      "\t\t2.14\tvintage\n",
      "\t\t2.14\tcell\n",
      "\t\t2.14\tlisting\n",
      "\t\t1.45\tphone\n",
      "\t\t1.22\tnew\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tvintage\n",
      "\t\t1.10\tcell\n",
      "\t\t1.10\tphone\n",
      "\t\t1.10\tflip\n",
      "\t\t0.41\tnew\n",
      "\t\t0.41\tlisting\n",
      "\n",
      "Kitchen Confidential by Anthony Bourdain FREE SHIPPING a paperback book\n",
      "books\n",
      "\t CountVectorizer\n",
      "\t\t1.00\tby\n",
      "\t\t1.00\ta\n",
      "\t\t1.00\tshipping\n",
      "\t\t1.00\tbook\n",
      "\t\t1.00\tfree\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tby\n",
      "\t\t1.00\ta\n",
      "\t\t1.00\tshipping\n",
      "\t\t1.00\tbook\n",
      "\t\t1.00\tfree\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tby\n",
      "\t\t2.83\ta\n",
      "\t\t2.83\tshipping\n",
      "\t\t2.83\tfree\n",
      "\t\t1.45\tbook\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tby\n",
      "\t\t1.10\ta\n",
      "\t\t1.10\tshipping\n",
      "\t\t1.10\tbook\n",
      "\t\t1.10\tfree\n",
      "\n",
      "Lanes Calm Life Nutrition Supplement For Relaxation And Tranquility Capsules\n",
      "nutrition\n",
      "\t CountVectorizer\n",
      "\t\t1.00\tcapsules\n",
      "\t\t1.00\tsupplement\n",
      "\t\t1.00\tfor\n",
      "\t\t1.00\tnutrition\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tcapsules\n",
      "\t\t1.00\tsupplement\n",
      "\t\t1.00\tfor\n",
      "\t\t1.00\tnutrition\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tsupplement\n",
      "\t\t2.83\tfor\n",
      "\t\t2.14\tcapsules\n",
      "\t\t1.22\tnutrition\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tcapsules\n",
      "\t\t1.10\tsupplement\n",
      "\t\t1.10\tfor\n",
      "\t\t1.10\tnutrition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "from Dataset import ads\n",
    "\n",
    "from Vectorizer import *\n",
    "\n",
    "random.seed(3)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(),\n",
    "        DictionaryVectorizer(),\n",
    "        TfidfVectorizer(),\n",
    "        TfidfVectorizer(group_by_class=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Prepare training and test sets\n",
    "    doc_ids__by__label = collections.defaultdict(list)\n",
    "    for i, (_, label) in enumerate(ads):\n",
    "        doc_ids__by__label[label].append(i)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = [], [], [], []\n",
    "    for label, doc_ids in doc_ids__by__label.items():\n",
    "        i = random.choice(list(range(len(doc_ids))))\n",
    "        doc_id = doc_ids.pop(i)\n",
    "        document, _ = ads[doc_id]\n",
    "        X_test.append(document)\n",
    "        Y_test.append(label)\n",
    "\n",
    "        X_train.extend([\n",
    "            ads[_doc_id][0] for _doc_id in doc_ids\n",
    "            if _doc_id != doc_id\n",
    "        ])\n",
    "\n",
    "        Y_train.extend([\n",
    "            label for _doc_id in doc_ids\n",
    "             if _doc_id != doc_id\n",
    "        ])\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit all the vectorizers on the same dataset\n",
    "    for vec in vectorizers:\n",
    "        vec.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Vectorize with each one and compare the results\n",
    "    for doc, label in zip(X_test, Y_test):\n",
    "        print(doc)\n",
    "        print(label)\n",
    "        for vec in vectorizers:\n",
    "            v = vec.transform([doc])[0]\n",
    "            print('\\t', vec)\n",
    "            for name, weight in vec.interpret(v):\n",
    "                print('\\t\\t%.2f\\t%s' % (weight, name))\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

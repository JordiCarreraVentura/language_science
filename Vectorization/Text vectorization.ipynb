{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-fashion",
   "metadata": {},
   "source": [
    "# Vectorization of short texts\n",
    "\n",
    "Currently, many NLP tasks receive as input tweets or tweet-like messages, that is, short texts that usually correspond to a single sentence. Think of e-commerce product titles, questions in question-answering systems, requests in intent detection, or individual sentences for a common use case of SEQ2SEQ and Transformer-based sentence encoders.\n",
    "\n",
    "However, standard bag-of-word (BoW) representations, as originally developed in the area of Information Retrieval and later adopted for Natural Language Processing, generally make the assumption that a system's input are whole documents, not isolated sentences, and longer documents providing a far larger context for language processing than sentences only.\n",
    "\n",
    "For instance, Latent-Dirichlet-Allocation (LDA)-based topic modelling was initially designed to represent the meaning of each word based on the meaning of its neighboring words. As such, long documents provided a much richer context from which to derive this kind of inferences, whereas short texts are usually too fragmented to allow or effective modelling using the same technique, and LDA is well-known to struggle with short documents.\n",
    "\n",
    "So, what is the specific impact of short texts on vectorization? If BoW representations are intended for longer texts, what happens if we suddenly start using them for shorter texts? Does it matter? Or does everything remain the same?\n",
    "\n",
    "In this notebook we want to show that it generally does not matter, except in one possible situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-discipline",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For our example, we will consider a small toy dataset consisting of 3 classes\n",
    "\n",
    "`Y = {\"cell-phones\", \"books\", \"nutrition\"}`\n",
    "\n",
    "with a few documents each:\n",
    "1. 9 documents for `cell-phones`\n",
    "2. 6 documents for `nutrition`\n",
    "3. 6 documents or `books`\n",
    "\n",
    "The dataset is hard-coded as variable `ads` inside the `Dataset` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-designation",
   "metadata": {},
   "source": [
    "## Vectorizers\n",
    "\n",
    "We will compare four types of vectorization strategies:\n",
    "1. Dictionary vectorization\n",
    "2. Frequency vectorization\n",
    "3. TFIDF-weighted vectorization\n",
    "4. TFICF-weighted vectorization\n",
    "\n",
    "All of these strategies are implemented as Python classes in the `Vectorizer` module under the following names, respectively:\n",
    "1. `DictionaryVectorizer`\n",
    "2. `CountVectorizer`\n",
    "3. `TfidfVectorizer` and 4 (`TfidfVectorizer` with keyword argument `group_by_class` set to `True`)\n",
    "\n",
    "\n",
    "First, let's define vectorizers for all the options we have discussed:\n",
    "CountVectorizer, TfidfVectorizer, DictionaryVectorizer.\n",
    "\n",
    "I'm not using scikit-learn's versions because I prefer to implement some\n",
    "#Â custom functionality directly from scratch (scikit-learn's vectorizers could\n",
    "also be extended accordingly, though), notice the `group_by_class` parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-homeless",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dramatic-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ericsson DF688 Vintage Flip Cell Phone NEW LISTING Ericsson DF688 Vintage Flip Cell Phone\n",
      "cell-phones\n",
      "\t CountVectorizer\n",
      "\t\t2.00\tvintage\n",
      "\t\t2.00\tcell\n",
      "\t\t2.00\tphone\n",
      "\t\t2.00\tflip\n",
      "\t\t1.00\tnew\n",
      "\t\t1.00\tlisting\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tnew\n",
      "\t\t1.00\tvintage\n",
      "\t\t1.00\tcell\n",
      "\t\t1.00\tphone\n",
      "\t\t1.00\tlisting\n",
      "\t\t1.00\tflip\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tflip\n",
      "\t\t2.14\tvintage\n",
      "\t\t2.14\tcell\n",
      "\t\t2.14\tlisting\n",
      "\t\t1.45\tphone\n",
      "\t\t1.22\tnew\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tvintage\n",
      "\t\t1.10\tcell\n",
      "\t\t1.10\tphone\n",
      "\t\t1.10\tflip\n",
      "\t\t0.41\tnew\n",
      "\t\t0.41\tlisting\n",
      "\n",
      "Kitchen Confidential by Anthony Bourdain FREE SHIPPING a paperback book\n",
      "books\n",
      "\t CountVectorizer\n",
      "\t\t1.00\tby\n",
      "\t\t1.00\ta\n",
      "\t\t1.00\tshipping\n",
      "\t\t1.00\tbook\n",
      "\t\t1.00\tfree\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tby\n",
      "\t\t1.00\ta\n",
      "\t\t1.00\tshipping\n",
      "\t\t1.00\tbook\n",
      "\t\t1.00\tfree\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tby\n",
      "\t\t2.83\ta\n",
      "\t\t2.83\tshipping\n",
      "\t\t2.83\tfree\n",
      "\t\t1.45\tbook\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tby\n",
      "\t\t1.10\ta\n",
      "\t\t1.10\tshipping\n",
      "\t\t1.10\tbook\n",
      "\t\t1.10\tfree\n",
      "\n",
      "Lanes Calm Life Nutrition Supplement For Relaxation And Tranquility Capsules\n",
      "nutrition\n",
      "\t CountVectorizer\n",
      "\t\t1.00\tcapsules\n",
      "\t\t1.00\tsupplement\n",
      "\t\t1.00\tfor\n",
      "\t\t1.00\tnutrition\n",
      "\t DictionaryVectorizer\n",
      "\t\t1.00\tcapsules\n",
      "\t\t1.00\tsupplement\n",
      "\t\t1.00\tfor\n",
      "\t\t1.00\tnutrition\n",
      "\t TfidfVectorizer\n",
      "\t\t2.83\tsupplement\n",
      "\t\t2.83\tfor\n",
      "\t\t2.14\tcapsules\n",
      "\t\t1.22\tnutrition\n",
      "\t TfidfVectorizer\n",
      "\t\t1.10\tcapsules\n",
      "\t\t1.10\tsupplement\n",
      "\t\t1.10\tfor\n",
      "\t\t1.10\tnutrition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "from Dataset import ads\n",
    "\n",
    "from Vectorizer import *\n",
    "\n",
    "random.seed(3)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(),\n",
    "        DictionaryVectorizer(),\n",
    "        TfidfVectorizer(),\n",
    "        TfidfVectorizer(group_by_class=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Prepare training and test sets\n",
    "    doc_ids__by__label = collections.defaultdict(list)\n",
    "    for i, (_, label) in enumerate(ads):\n",
    "        doc_ids__by__label[label].append(i)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = [], [], [], []\n",
    "    for label, doc_ids in doc_ids__by__label.items():\n",
    "        i = random.choice(list(range(len(doc_ids))))\n",
    "        doc_id = doc_ids.pop(i)\n",
    "        document, _ = ads[doc_id]\n",
    "        X_test.append(document)\n",
    "        Y_test.append(label)\n",
    "\n",
    "        X_train.extend([\n",
    "            ads[_doc_id][0] for _doc_id in doc_ids\n",
    "            if _doc_id != doc_id\n",
    "        ])\n",
    "\n",
    "        Y_train.extend([\n",
    "            label for _doc_id in doc_ids\n",
    "             if _doc_id != doc_id\n",
    "        ])\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit all the vectorizers on the same dataset\n",
    "    for vec in vectorizers:\n",
    "        vec.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Vectorize with each one and compare the results\n",
    "    for doc, label in zip(X_test, Y_test):\n",
    "        print(doc)\n",
    "        print(label)\n",
    "        for vec in vectorizers:\n",
    "            v = vec.transform([doc])[0]\n",
    "            print('\\t', vec)\n",
    "            for name, weight in vec.interpret(v):\n",
    "                print('\\t\\t%.2f\\t%s' % (weight, name))\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

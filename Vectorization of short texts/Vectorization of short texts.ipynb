{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-fashion",
   "metadata": {},
   "source": [
    "# Vectorization of short texts\n",
    "\n",
    "Currently, many NLP tasks receive as input tweets or tweet-like messages, that is, short texts that usually correspond to a single sentence. Think of e-commerce product titles, questions in question-answering systems, requests in intent detection, or individual sentences for a common use case of SEQ2SEQ and Transformer-based sentence encoders.\n",
    "\n",
    "However, standard bag-of-word (BoW) representations, as originally developed in the area of Information Retrieval and later adopted for Natural Language Processing, generally make the assumption that a system's input are whole documents, not isolated sentences, and longer documents providing a far larger context for language processing than sentences only.\n",
    "\n",
    "For instance, Latent-Dirichlet-Allocation (LDA)-based topic modelling was initially designed to represent the meaning of each word based on the meaning of its neighboring words. As such, long documents provided a much richer context from which to derive this kind of inferences, whereas short texts are usually too fragmented to allow or effective modelling using the same technique, and LDA is well-known to struggle with short documents.\n",
    "\n",
    "So, what is the specific impact of short texts on vectorization? If BoW representations are intended for longer texts, what happens if we suddenly start using them for shorter texts? Does it matter? Or does everything remain the same?\n",
    "\n",
    "In this notebook we want to show that it generally does not matter, except in one possible situation.\n",
    "\n",
    "For our experiment, we will need to\n",
    "1. define the dataset we will be using,\n",
    "2. define which text vectorization methods we want to compare;\n",
    "3. and assess the differences between each method when run on the given dataset.\n",
    "\n",
    "Points 1 and 2 will be covered next. After that, we will use them as inputs for the analysis in point 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-discipline",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "For our example, we will consider a small toy dataset consisting of 3 classes\n",
    "\n",
    "`Y = {\"cell-phones\", \"books\", \"nutrition\"}`\n",
    "\n",
    "with a few documents each:\n",
    "1. 9 documents for `cell-phones`\n",
    "2. 6 documents for `nutrition`\n",
    "3. 6 documents for `books`\n",
    "\n",
    "The dataset is hard-coded as variable `ads` inside the `Dataset` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-designation",
   "metadata": {},
   "source": [
    "## 2. Vectorizers\n",
    "\n",
    "We will consider four types of vectorization strategies:\n",
    "1. Dictionary vectorization\n",
    "2. Frequency vectorization\n",
    "3. TFIDF-weighted vectorization\n",
    "4. TFICF-weighted vectorization\n",
    "\n",
    "All of these strategies are implemented as Python classes in the `Vectorizer` module under the following names, respectively:\n",
    "1. `DictionaryVectorizer`\n",
    "2. `CountVectorizer`\n",
    "3. `TfidfVectorizer` and 4 (`TfidfVectorizer` with keyword argument `group_by_class` set to `True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-tiffany",
   "metadata": {},
   "source": [
    "### 2.1. Technical specifications\n",
    "Here is the one-paragraph description of each of these methods:\n",
    "\n",
    "#### 2.1.1. `DictionaryVectorizer`\n",
    "The `DictionaryVectorizer` will take each document and return its binary encoding, that is, a vector\n",
    "1. with as many columns as words are in our vocabulary (where vocabulary is defined as the set of all unique words occurring in our documents) and,\n",
    "2. for each column, either a 1 or a 0, depending on whether the document contains that word.\n",
    "   \n",
    "This is esentially an implementation of the identity function over a vocabulary and over all the words in an input text. The output should the same as if we used the `scikit-learn.feature_extraction.text.CountVectorizer` class with the `binary` parameter set to `True`, for those who are familiar with Python's `scikit-learn` library.\n",
    "\n",
    "#### 2.1.2. `CountVectorizer`\n",
    "The `CountVectorizer` also returns a vector for each input document and with as many columns as words are in our vocabulary. However, in this case the value of each column is **not** 1 or 0, but actually the **frequency** of a word in the input text (each column of the vector corresponds to one of the words in our vocabulary, and only that word). So, the values in the columns can be 0, 1, or any higher natural number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-division",
   "metadata": {},
   "source": [
    "#### 2.1.3. `TfidfVectorizer`\n",
    "The `TfidfVectorizer` returns a vector where, instead of a categorical binary label or a frequency count, each column contains the TFIDF weight for that term.\n",
    "\n",
    "What is the TFIDF weight? [Here's](https://en.wikipedia.org/wiki/Tf–idf) the technical definition, but as a quick summary, it is a measure derived\n",
    "1. from the word's **frequency** as measured over a document (the TF part of the TFIDF equation),\n",
    "2. which is then multiplied times that word's frequency over all the documents and, more precisely, by its **inverse** frequency over all the documents (the IDF part of the TFIDF equation).\n",
    "\n",
    "**Why is the *inverse frequency* used?**\n",
    "\n",
    "If a word appears in **every** document, it is too general and it is not informative, that is, it is not meaningfully associated with any particular document. This means that, if we used that word to try to find a document, we would end up with the whole set of documents! Put another way, those words contribute little to a document search and, in tasks where the goal is to filter a large initial search space down to a few relevant docments, those words will have no effect, proving to be redundant at best, and confounders in the worst case scenario.\n",
    "\n",
    "That is why, in the context of Document Retrieval (where BoW representations were first introduced, and where the goal is to search for specific documents over a database), it is best to simply filter out these words. In this paradigm, the best way to find any document (and to represent the meaning of that document, since meaning and searchable content are taken to be the same) is to find the words\n",
    "1. with the highest frequency in the document (TF) (since these terms can be expected to denote what that document is about: a text about politics should contain **many** occurrences of words like *government*, *president* or *country*) and \n",
    "2. the lowest frequency over all documents (IDF) (since such words would denote what only a few documents are about, and those are the document we need): if a word appears in a single document, and somebody searches for that word, there is no doubt as to what document they are looking for or, at least, what is the only document that can be returned as a candidate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-easter",
   "metadata": {},
   "source": [
    "#### 2.1.4. `TfidfVectorizer` with inverse class frequency (instead of inverse document frequency)\n",
    "Finally, the `TfidfVectorizer` is almost the same as the `TfidfVectorizer`, with a major difference:\n",
    "- it also uses the TF term of the TFIDF equation as defined above\n",
    "- but it modifies the IDF term so that it is computed over classes of documents instead of all the documents.\n",
    "\n",
    "That is, given the standard X and Y axis of an annotated dataset, such that $\\exists y: y \\in Y \\land \\sum_{i}^{|Y|} \\big\\{ 1\\text{ if }Y_i \\neq y\\text{ else 0} \\big\\} \\ge 1$, instead of using each _x_ in X as the documents for the IDF calculation, we will modify the definition and consider as a document the concatenation of every _x_ with the same _y_.\n",
    "\n",
    "Given this requirement, this vectorization is only possible when the vector of Y labels is given. This is (always?) the case in the context of a supervised machine learning task, but differs from e.g. `scikit-learn`'s standard assumptions, whereby their vectorizers ignore the Y axis of the input matrix.\n",
    "\n",
    "However, if this data is indeed given to us during training, we can leverage it to compute the TFIDF for classes of documents rather than for every document individually.\n",
    "\n",
    "**And is that a good thing?**\n",
    "\n",
    "1. Not necessarily in the context of an Information Retrieval task (where document-specific calculation will always return the set of key words that answer a query with the highest precision)\n",
    "2. but probably yes for other tasks, since another consequence of traditional TFIDF is that it assigns comparatively lower scores to class-defining key words (due to their lower IDFs), which can be particularly damaging for imbalanced datasets (if a class contains many more documents than the rest, all its key words will be assigned weights closer to those of stop words than to those of key words from other classes, penalizing the best features for predicting the majority class just by virtue of it being the majority class).\n",
    "\n",
    "**Example**\n",
    "\n",
    "To illustrate our point, imagine we are classifying political articles: names of politicians mentioned in a single story will have a very high IDF, whereas the name of a politician who is always mentioned on political news stories would have a much lower value despite being far more central to our domain. This is the equivalent of Barak Obama being assigned a lower weight in the domain of politics than any local leader he once met with.\n",
    "\n",
    "That local leader's name is definitely the best cue we could provide the system if we wanted to find documents about that person but, if we are interested in a representation of the domain of politics, assigning a higher weight to those outliers (possibly leading to overfitting) as opposed to a core domain entity like Barak Obama for the topic of politics, seems clearly suboptimal.\n",
    "\n",
    "In its standard definition, TFIDF favors specificity at the expense of representativity, returning the most direct answer to the question, but not necessarily the best answer overall. When applied as an input representation method, it results in underestimation of all the central categories in the semantic space we are considering, missing the forest for the trees.\n",
    "\n",
    "The modified class-level TFIDF, or TFICF for short, should provide a slightly better representation whenever the Y axis is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-nicaragua",
   "metadata": {},
   "source": [
    "## 3. Working hypothesis\n",
    "Based on these descriptions, we can already venture some hypotheses:\n",
    "1. `CountVectorizer` represents essentially the same information as the `DictionaryVectorizer` but with raw frequency counts instead of a categorial binary labeling. If we are working with long documents containing many mentions of the same words, then the values in `CountVectorizer`'s vector would be much larger than the values in `DictionaryVectorizer`'s vector, since the latter are effectively capped to 1 no matter how many times each word appears in the text. However, if we are working with short texts, which will tend to contain only one occurrence of each word (except for prepositions, determiners, and similar function words with little lexical meaning), frequency becomes irrelevant, as it will always be either 0 or 1, becoming essentially binary as well, the same set of values used by the `DictionaryVectorizer`. Therefore, our hypothesis is that, with most words having the same frequency in short texts, **a short text's** `DictionaryVectorizer` **-encoded vector will look very similar, if not identical, to a** `CountVectorizer`**-encoded vector** (which is definitely **not** true for long documents).\n",
    "2. The two implementations of `TfidfVectorizer` are also very similar, with both using the same formula to calculate TFIDF, and with the only difference being what each of them takes as the \"document\", its unit of analysis for the IDF term, which can be either\n",
    "  - each individual document (for the standard vectorizer),\n",
    "  - each class (represented as the concatenation of all its documents) or\n",
    "  - some number in between, by randomly grouping documents from the same class into an arbitrary subclass, which results in a number of documents that is less than the original but still greater than the number of classes (which helps in binary classification tasks, where having two classes only would make the IDF scores too close).\n",
    "  \n",
    "  By grouping the documents in this way, we are shifting the IDF penalty\n",
    "  - from words that occur in many documents (whether it's a stop word like _the_ or a content word like `Barak Obama`)\n",
    "  - to words that occur in many classes, regardless of the number of documents,\n",
    "  since, what we should realize penalize is the latter (because the fact that a word appears with many different classes means it is a constant, not an independent variable that can help us predict those classes –and, in this case, whereas `the` will still meet this definition, given that it will appear in every class, `Barak Obama` will not, given that it will mainly occur in texts about politics).\n",
    "\n",
    "  Therefore, **we expect class-defining key words, penalized by the traditional TFIDF-weighting scheme, to have higher weights using the modified TFICF weighting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-homeless",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "### Python pipeline\n",
    "\n",
    "Let's start by importing the dependencies and all necessary objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dramatic-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "from Dataset import ads\n",
    "\n",
    "from Vectorizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-metallic",
   "metadata": {},
   "source": [
    "Let's seed the random algorithm, for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alert-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-diving",
   "metadata": {},
   "source": [
    "Next, we initialize the objects corresponding to all the vectorizers we'll compare, the same we introduced in section 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alpine-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [\n",
    "    CountVectorizer(),\n",
    "    DictionaryVectorizer(),\n",
    "    TfidfVectorizer(),\n",
    "    TfidfVectorizer(group_by_class=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-avatar",
   "metadata": {},
   "source": [
    "And then, we split the data into training and test set. We will take 1 random instance as the test set, and use the remaining instances of each class as the training set to fit each vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "formal-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids__by__label = collections.defaultdict(list)\n",
    "for i, (_, label) in enumerate(ads):\n",
    "    doc_ids__by__label[label].append(i)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = [], [], [], []\n",
    "for label, doc_ids in doc_ids__by__label.items():\n",
    "    i = random.choice(list(range(len(doc_ids))))\n",
    "    doc_id = doc_ids.pop(i)\n",
    "    document, _ = ads[doc_id]\n",
    "    X_test.append(document)\n",
    "    Y_test.append(label)\n",
    "\n",
    "    X_train.extend([\n",
    "        ads[_doc_id][0] for _doc_id in doc_ids\n",
    "        if _doc_id != doc_id\n",
    "    ])\n",
    "\n",
    "    Y_train.extend([\n",
    "        label for _doc_id in doc_ids\n",
    "         if _doc_id != doc_id\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-jerusalem",
   "metadata": {},
   "source": [
    "Having defined the training and test sets, we can finally fit each vectorizer, and then use them on the test set to get the feature weights and compare them across the different schemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "minimal-fairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT DOCUMENT: \"Ericsson DF688 Vintage Flip Cell Phone NEW LISTING Ericsson DF688 Vintage Flip Cell Phone\"\n",
      "\n",
      "DOCUMENT LABEL: cell-phones\n",
      "\n",
      "  VECTORIZER TYPE: CountVectorizer\n",
      "\tweight=2.00\tfeature=\"vintage\"\n",
      "\tweight=2.00\tfeature=\"cell\"\n",
      "\tweight=2.00\tfeature=\"phone\"\n",
      "\tweight=2.00\tfeature=\"flip\"\n",
      "\tweight=1.00\tfeature=\"new\"\n",
      "\tweight=1.00\tfeature=\"listing\"\n",
      "\n",
      "  VECTORIZER TYPE: DictionaryVectorizer\n",
      "\tweight=1.00\tfeature=\"new\"\n",
      "\tweight=1.00\tfeature=\"vintage\"\n",
      "\tweight=1.00\tfeature=\"cell\"\n",
      "\tweight=1.00\tfeature=\"phone\"\n",
      "\tweight=1.00\tfeature=\"listing\"\n",
      "\tweight=1.00\tfeature=\"flip\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=2.83\tfeature=\"flip\"\n",
      "\tweight=2.14\tfeature=\"vintage\"\n",
      "\tweight=2.14\tfeature=\"cell\"\n",
      "\tweight=2.14\tfeature=\"listing\"\n",
      "\tweight=1.45\tfeature=\"phone\"\n",
      "\tweight=1.22\tfeature=\"new\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=1.10\tfeature=\"vintage\"\n",
      "\tweight=1.10\tfeature=\"cell\"\n",
      "\tweight=1.10\tfeature=\"phone\"\n",
      "\tweight=1.10\tfeature=\"flip\"\n",
      "\tweight=0.41\tfeature=\"new\"\n",
      "\tweight=0.41\tfeature=\"listing\"\n",
      "\n",
      "\n",
      "\n",
      "INPUT DOCUMENT: \"Kitchen Confidential by Anthony Bourdain FREE SHIPPING a paperback book\"\n",
      "\n",
      "DOCUMENT LABEL: books\n",
      "\n",
      "  VECTORIZER TYPE: CountVectorizer\n",
      "\tweight=1.00\tfeature=\"by\"\n",
      "\tweight=1.00\tfeature=\"a\"\n",
      "\tweight=1.00\tfeature=\"shipping\"\n",
      "\tweight=1.00\tfeature=\"book\"\n",
      "\tweight=1.00\tfeature=\"free\"\n",
      "\n",
      "  VECTORIZER TYPE: DictionaryVectorizer\n",
      "\tweight=1.00\tfeature=\"by\"\n",
      "\tweight=1.00\tfeature=\"a\"\n",
      "\tweight=1.00\tfeature=\"shipping\"\n",
      "\tweight=1.00\tfeature=\"book\"\n",
      "\tweight=1.00\tfeature=\"free\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=2.83\tfeature=\"by\"\n",
      "\tweight=2.83\tfeature=\"a\"\n",
      "\tweight=2.83\tfeature=\"shipping\"\n",
      "\tweight=2.83\tfeature=\"free\"\n",
      "\tweight=1.45\tfeature=\"book\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=1.10\tfeature=\"by\"\n",
      "\tweight=1.10\tfeature=\"a\"\n",
      "\tweight=1.10\tfeature=\"shipping\"\n",
      "\tweight=1.10\tfeature=\"book\"\n",
      "\tweight=1.10\tfeature=\"free\"\n",
      "\n",
      "\n",
      "\n",
      "INPUT DOCUMENT: \"Lanes Calm Life Nutrition Supplement For Relaxation And Tranquility Capsules\"\n",
      "\n",
      "DOCUMENT LABEL: nutrition\n",
      "\n",
      "  VECTORIZER TYPE: CountVectorizer\n",
      "\tweight=1.00\tfeature=\"capsules\"\n",
      "\tweight=1.00\tfeature=\"supplement\"\n",
      "\tweight=1.00\tfeature=\"for\"\n",
      "\tweight=1.00\tfeature=\"nutrition\"\n",
      "\n",
      "  VECTORIZER TYPE: DictionaryVectorizer\n",
      "\tweight=1.00\tfeature=\"capsules\"\n",
      "\tweight=1.00\tfeature=\"supplement\"\n",
      "\tweight=1.00\tfeature=\"for\"\n",
      "\tweight=1.00\tfeature=\"nutrition\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=2.83\tfeature=\"supplement\"\n",
      "\tweight=2.83\tfeature=\"for\"\n",
      "\tweight=2.14\tfeature=\"capsules\"\n",
      "\tweight=1.22\tfeature=\"nutrition\"\n",
      "\n",
      "  VECTORIZER TYPE: TfidfVectorizer\n",
      "\tweight=1.10\tfeature=\"capsules\"\n",
      "\tweight=1.10\tfeature=\"supplement\"\n",
      "\tweight=1.10\tfeature=\"for\"\n",
      "\tweight=1.10\tfeature=\"nutrition\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit all the vectorizers on the same dataset\n",
    "for vec in vectorizers:\n",
    "    vec.fit(X_train, Y_train)\n",
    "\n",
    "# Vectorize with each one and compare the results\n",
    "for doc, label in zip(X_test, Y_test):\n",
    "    print('INPUT DOCUMENT: \"%s\"' % doc)\n",
    "    print('\\nDOCUMENT LABEL: %s' % label)\n",
    "    for vec in vectorizers:\n",
    "        v = vec.transform([doc])[0]\n",
    "        print('\\n  VECTORIZER TYPE: %s' % vec)\n",
    "        for name, weight in vec.interpret(v):\n",
    "            print('\\tweight=%.2f\\tfeature=\"%s\"' % (weight, name))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-samuel",
   "metadata": {},
   "source": [
    "As shown above, the results confirm our two hypotheses, 3.1 and 3.2. More specifically,\n",
    "1. with regard to hypothesis 3.1, `CountVectorizer` and `DictionaryVectorizer` return identical feature vectors in most cases (compare the features and their weights for labels `nutrition` and `books`). For short texts, therefore, the performance of both should be roughly the same;\n",
    "2. with regard to hypothesis 3.2, the TFICF version of `TfidfVectorizer` assigns higher weights to core class features, e.g.\n",
    "   1. in the `cell-phones` class, TFICF returns _vintage_, _cell_, _phone_ and _flip_ as the most-highly-weighted features, all within the same weight tier (top tier, with a score of 1.10), and they are all core concepts in the category `cell-phones`. Compare that to TFIDF, where a key word like _phone_ falls down to the 3rd tier, behind `vintage`, or lower even than `cell`, with which it should be essentially correlated;\n",
    "   2. in the `books` class, the word _book_ itself is in the 2nd weight tier for TFIDF but the 1st tier for TFICF;\n",
    "   3. in the `nutrition` class, the word _nutrition_ itself is in the 3rd tier for TIFDF but again the 1st for TFICF, along with the rest of category-defining key words such as _capsules_ and _supplement_. \n",
    "\n",
    "In all cases, the feature weights returned by TFICF seem to closer to our own intution as to which words best describe each of the classes and should be weighted accordingly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-senior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
